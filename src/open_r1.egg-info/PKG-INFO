Metadata-Version: 2.2
Name: open-r1
Version: 0.1.0.dev0
Summary: Open R1
Home-page: https://github.com/huggingface/open-r1
Author: The Hugging Face team (past and future)
Author-email: lewis@huggingface.co
License: Apache
Keywords: llm inference-time compute reasoning
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10.9
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: accelerate==1.4.0
Requires-Dist: bitsandbytes>=0.43.0
Requires-Dist: einops>=0.8.0
Requires-Dist: datasets>=3.2.0
Requires-Dist: deepspeed==0.15.4
Requires-Dist: hf_transfer>=0.1.4
Requires-Dist: huggingface-hub[cli]<1.0,>=0.19.2
Requires-Dist: langdetect
Requires-Dist: latex2sympy2_extended>=1.0.6
Requires-Dist: math-verify==0.5.2
Requires-Dist: liger_kernel==0.5.3
Requires-Dist: packaging>=23.0
Requires-Dist: safetensors>=0.3.3
Requires-Dist: sentencepiece>=0.1.99
Requires-Dist: transformers==4.49.0
Requires-Dist: trl@ git+https://github.com/huggingface/trl.git@69ad852e5654a77f1695eb4c608906fe0c7e8624
Requires-Dist: wandb>=0.19.1
Provides-Extra: tests
Requires-Dist: pytest; extra == "tests"
Requires-Dist: parameterized>=0.9.0; extra == "tests"
Requires-Dist: math-verify==0.5.2; extra == "tests"
Provides-Extra: torch
Requires-Dist: torch==2.5.1; extra == "torch"
Provides-Extra: quality
Requires-Dist: ruff>=0.9.0; extra == "quality"
Requires-Dist: isort>=5.12.0; extra == "quality"
Requires-Dist: flake8>=6.0.0; extra == "quality"
Provides-Extra: code
Requires-Dist: e2b-code-interpreter>=1.0.5; extra == "code"
Requires-Dist: python-dotenv; extra == "code"
Provides-Extra: eval
Requires-Dist: lighteval@ git+https://github.com/huggingface/lighteval.git@ed084813e0bd12d82a06d9f913291fdbee774905 ; extra == "eval"
Requires-Dist: math-verify==0.5.2; extra == "eval"
Provides-Extra: dev
Requires-Dist: ruff>=0.9.0; extra == "dev"
Requires-Dist: isort>=5.12.0; extra == "dev"
Requires-Dist: flake8>=6.0.0; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Requires-Dist: parameterized>=0.9.0; extra == "dev"
Requires-Dist: math-verify==0.5.2; extra == "dev"
Requires-Dist: lighteval@ git+https://github.com/huggingface/lighteval.git@ed084813e0bd12d82a06d9f913291fdbee774905 ; extra == "dev"
Requires-Dist: math-verify==0.5.2; extra == "dev"
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: keywords
Dynamic: license
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models


<p align="center">
CPPO-1.5B-n-16-0.875 <a href="https://huggingface.co/Stardust1956/CPPO-1.5B-n-16-0.875">ü§ó</a> <br>
<a href="https://arxiv.org/abs/2503.22342"> <img src='https://img.shields.io/badge/arXiv-2503.22342-b31b1b.svg'></a> <br> Wechat <a href="#discussion">üí≠</a> <br>

<div align="center">
    <h2>If you like CPPO, pls give us a star‚≠ê </h2>
</div>

</p>

## Abstract
This paper introduces Completion Pruning Policy Optimization (CPPO) to accelerate the training of reasoning models based on Group Relative Policy Optimization (GRPO). GRPO, while effective, incurs high training costs due to the need for sampling multiple completions for each question. Our experiment and theoretical analysis reveals that the number of completions impacts model accuracy yet increases training time multiplicatively, and **not all completions contribute equally to policy training**---t**heir contribution depends on their relative advantage**. To address these issues, we propose CPPO, which prunes completions with low absolute advantages, significantly reducing the number needed for gradient calculation and updates. Additionally, we introduce a dynamic completion allocation strategy to maximize GPU utilization by incorporating additional questions, further enhancing training efficiency. Experimental results demonstrate that CPPO achieves up to **$8.32\times$** speedup on GSM8K and **$3.51\times$** on Math while preserving or even enhancing the accuracy compared to the original GRPO.

## To Do
- [ ] The training code of CPPO in Math dataset.

## Main Results
| Method                | Group Size (G) | Pruning Rate (P) | k  | Accuracy  | Training Time | Accelerate Ratio |
|-----------------------|---------------|------------------|----|-----------|---------------|------------------|
| Qwen2.5-1.5B-Instruct | -             | -                | -  | 55.72%    | -             | -                |
| GRPO                 | 16            | 0.00%            | 16 | 77.05%    | 23393s        | 1.00√ó            |
| CPPO                 | 16            | 50.00%           | 8  | 77.67%    | 12930s        | 1.81√ó            |
| CPPO                 | 16            | 75.00%           | 4  | 78.81%    | 7159s         | 3.27√ó            |
| CPPO                 | 16            | 87.50%           | 2  | 80.41%    | 4781s         | 4.89√ó            |
| CPPO                 | 16            | 93.75%           | 1  | 78.20%    | 2813s         | 8.32√ó            |


## To Reproduce

### 1. Prepare the environment according to [Open R1](https://github.com/huggingface/open-r1) or :
```bash
conda create -n cppo python=3.11
conda activate cppo
pip install vllm==0.7.2
pip install setuptools
pip install flash-attn --no-build-isolation
pip install -e ".[dev]"
```
### 2. Training:
You need two GPU with 80G memory to reproduce our results on GSM8K.
#### GRPO
```bash
sh scripts/GRPO.sh
```
#### CPPO
```bash
sh scripts/CPPO.sh
```
### 3. Evaluation:
#### Qwen2.5-1.5B-Instruct
```bash
sh scripts/Eval_qwen2.5-1.5b.sh
```
#### CPPO-1.5B-n-16-0.875
```bash
sh scripts/Eval.sh
```


## Acknowledgments
We are very grateful to the [Open R1](https://github.com/huggingface/open-r1) teams for creating awesome repo.

## Discussion
If you are interested in this project, please feel free to join our Wechat group for discussion.

<img src="./asset/wechat_new.jpg" width="300">

## Citation
```bibtex
@article{lin2025cppo,
  title={CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models},
  author={Lin, Zhihang and Lin, Mingbao and Xie, Yuan and Ji, Rongrong},
  journal={arXiv preprint arXiv:2503.22342},
  year={2025}
}
```
